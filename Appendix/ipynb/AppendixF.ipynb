{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\myvector}[2]{\\left\\{ \\begin{array}{c}#1\\\\#2\\end{array} \\right\\}}$\n",
    "$\\newcommand{\\mymatrix}[4]{\\left[ \\begin{array}{c}#1&#2\\\\#3&#4\\end{array} \\right]}$\n",
    "$\\newcommand{\\sM}[1]{\\boldsymbol{s}_{\\mathrm{M#1}}}$\n",
    "$\\newcommand{\\rvector}[1]{\\boldsymbol{r}_{\\mathrm{#1}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX F:  Effect of informative priors\n",
    "\n",
    "\n",
    "This Appendix revists Appendix D to consider the effect of informative priors.\n",
    "\n",
    "Let's first assemble all relevant functions from Appendix D. Of these functions, the only function that has been changed relative to Appendix D is `solution_bayesian`, which here offer three sets of prior distribution options: (1) vague, (2) informative and (3) highly informative.\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import sin,cos,radians,degrees\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import pymc\n",
    "\n",
    "\n",
    "def rotation_matrix(theta):\n",
    "    '''Construct rotation matrix'''\n",
    "    c,s    = cos(theta), sin(theta)\n",
    "    R      = np.matrix(  [[c, -s], [s, c]] )\n",
    "    return R\n",
    "\n",
    "\n",
    "def rotate(R, r):\n",
    "    '''Rotate a position vector r using rotation matrix R'''\n",
    "    return np.asarray(  (R * np.mat(r).T)  ).flatten()\n",
    "\n",
    "\n",
    "def get_positions(y, theta):\n",
    "    '''Compute global marker positions given y and theta'''\n",
    "    rA     = np.array([0, y])       #global A position\n",
    "    R      = rotation_matrix(theta)\n",
    "    rM1    = rA + rotate(R, sM1)    #global M1 position\n",
    "    rM2    = rA + rotate(R, sM2)    #global M2 position\n",
    "    return rM1,rM2\n",
    "\n",
    "\n",
    "def measurement_error(x, q_obs):\n",
    "    y,theta = x\n",
    "    rpM1    = q_obs[:2]\n",
    "    rpM2    = q_obs[2:]\n",
    "    rM1,rM2 = get_positions(y, theta)\n",
    "    e1,e2   = rpM1 - rM1, rpM2 - rM2\n",
    "    e1,e2   = np.linalg.norm(e1), np.linalg.norm(e2)\n",
    "    f       = e1**2 + e2**2\n",
    "    return f\n",
    "\n",
    "\n",
    "def solution_ls(q_obs):\n",
    "    x0      = [y_true, theta_true]  #initial (y, theta) guess\n",
    "    results = optimize.minimize(measurement_error, x0, args=(q_obs,))\n",
    "    y,theta = results.x\n",
    "    return y, degrees(theta)\n",
    "\n",
    "\n",
    "def solution_bayesian(q_obs, prior=1):\n",
    "    '''\n",
    "    prior=1 :  vague prior\n",
    "    prior=2 :  informative prior (using LS solution;  same as in Appendix D)\n",
    "    prior=3 :  highly informative prior\n",
    "    '''\n",
    "    \n",
    "    if prior==1:    # vague prior\n",
    "        tau     = pymc.Uniform(\"tau\", 0, 10)\n",
    "        y       = pymc.Uniform(\"y\", -10, 10)\n",
    "        theta   = pymc.Uniform(\"theta\", -radians(90), radians(90))\n",
    "    \n",
    "    elif prior==2:  # LS-informed prior\n",
    "        qls     = solution_ls( q_obs )\n",
    "        yls     = qls[0]              \n",
    "        thetals = radians( qls[1] )   \n",
    "        tau     = pymc.Uniform(\"tau\", 0, 2 * 1/(noise_amp**2))\n",
    "        y       = pymc.Normal(\"y\", yls, 5, value=yls)    \n",
    "        theta   = pymc.Normal(\"theta\", thetals, radians(10), value=thetals)\n",
    "\n",
    "    elif prior==3:  # highly informative prior\n",
    "        tau     = pymc.Uniform(\"tau\", 0, 2 * 1/(noise_amp**2))\n",
    "        y       = pymc.Normal(\"y\", y_true, 5, value=y_true)\n",
    "        theta   = pymc.Normal(\"theta\", theta_true, radians(10), value=theta_true)\n",
    "\n",
    "\n",
    "    @pymc.deterministic\n",
    "    def observations_model(y=y, theta=theta):\n",
    "        rM1,rM2 = get_positions(y, theta)\n",
    "        q       = np.asarray([rM1, rM2]).flatten()\n",
    "        return q\n",
    "    q_model   = pymc.Normal(\"q\", observations_model, tau, value=q_obs, observed=True)\n",
    "\n",
    "    mcmc      = pymc.MCMC([q_model, y, theta, tau])\n",
    "    mcmc.sample(20000, 5000, progress_bar=False)\n",
    "    Y         = mcmc.trace('y')[:]\n",
    "    THETA     = np.degrees( mcmc.trace('theta')[:] )\n",
    "    \n",
    "    return Y.mean(), THETA.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Let's next calculate Bayesian solutions for all three sets of priors in `solution_bayesian` using the simulated the dataset from Appendix D.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 10...\n",
      "   y1 = -0.040,  theta1 = 5.014\n",
      "   y2 = -0.053,  theta2 = 5.026\n",
      "   y3 = -0.075,  theta3 = 5.056\n",
      "\n",
      "Iteration 2 of 10...\n",
      "   y1 = 0.397,  theta1 = 4.255\n",
      "   y2 = -0.061,  theta2 = 4.904\n",
      "   y3 = -0.007,  theta3 = 4.823\n",
      "\n",
      "Iteration 3 of 10...\n",
      "   y1 = 0.214,  theta1 = 4.628\n",
      "   y2 = -0.165,  theta2 = 5.161\n",
      "   y3 = -0.193,  theta3 = 5.200\n",
      "\n",
      "Iteration 4 of 10...\n",
      "   y1 = 1.005,  theta1 = 3.437\n",
      "   y2 = -0.038,  theta2 = 4.928\n",
      "   y3 = -0.006,  theta3 = 4.882\n",
      "\n",
      "Iteration 5 of 10...\n",
      "   y1 = 0.131,  theta1 = 4.629\n",
      "   y2 = 0.182,  theta2 = 4.558\n",
      "   y3 = -0.035,  theta3 = 4.867\n",
      "\n",
      "Iteration 6 of 10...\n",
      "   y1 = 0.745,  theta1 = 3.797\n",
      "   y2 = -0.054,  theta2 = 4.928\n",
      "   y3 = -0.078,  theta3 = 4.963\n",
      "\n",
      "Iteration 7 of 10...\n",
      "   y1 = 0.161,  theta1 = 4.562\n",
      "   y2 = -0.100,  theta2 = 4.942\n",
      "   y3 = -0.084,  theta3 = 4.920\n",
      "\n",
      "Iteration 8 of 10...\n",
      "   y1 = 0.065,  theta1 = 4.819\n",
      "   y2 = 0.269,  theta2 = 4.536\n",
      "   y3 = 0.156,  theta3 = 4.695\n",
      "\n",
      "Iteration 9 of 10...\n",
      "   y1 = 0.391,  theta1 = 4.235\n",
      "   y2 = -0.529,  theta2 = 5.553\n",
      "   y3 = -0.492,  theta3 = 5.500\n",
      "\n",
      "Iteration 10 of 10...\n",
      "   y1 = 0.186,  theta1 = 4.626\n",
      "   y2 = 0.253,  theta2 = 4.523\n",
      "   y3 = 0.156,  theta3 = 4.662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### local positions:\n",
    "sM1        = np.array([35.0, 0.0])  # cm\n",
    "sM2        = np.array([45.0, 0.0])  # cm\n",
    "\n",
    "### true values:\n",
    "y_true      = -0.1\n",
    "theta_true  = radians(5)\n",
    "\n",
    "### true marker positions:\n",
    "rM1,rM2    = get_positions(y_true, theta_true) \n",
    "q_true     = np.array([rM1, rM2]).flatten()\n",
    "\n",
    "### measurements:\n",
    "rpM1       = np.array([33.51, 12.11])\n",
    "rpM2       = np.array([42.63, 16.18])\n",
    "\n",
    "### simulation parameters:\n",
    "np.random.seed(0)\n",
    "nIterations = 10\n",
    "noise_amp   = 0.05  # cm\n",
    "Q_obs       = q_true + noise_amp * np.random.randn(nIterations, 4)\n",
    "\n",
    "\n",
    "### run simulation:\n",
    "RESULTS_LS  = np.array( [solution_ls(q_obs)  for q_obs in Q_obs] )\n",
    "RESULTS_B1  = np.zeros( (nIterations,2) )\n",
    "RESULTS_B2  = np.zeros( (nIterations,2) )\n",
    "RESULTS_B3  = np.zeros( (nIterations,2) )\n",
    "for i,q_obs in enumerate(Q_obs):\n",
    "    print('Iteration %d of %d...' %(i+1, nIterations))\n",
    "    y1,theta1 = solution_bayesian(q_obs, prior=1)\n",
    "    y2,theta2 = solution_bayesian(q_obs, prior=2)\n",
    "    y3,theta3 = solution_bayesian(q_obs, prior=3)\n",
    "    RESULTS_B1[i] = [y1, theta1]\n",
    "    RESULTS_B2[i] = [y2, theta2]\n",
    "    RESULTS_B3[i] = [y3, theta3]\n",
    "    print('   y1 = %.3f,  theta1 = %.3f' %(y1,theta1))\n",
    "    print('   y2 = %.3f,  theta2 = %.3f' %(y2,theta2))\n",
    "    print('   y3 = %.3f,  theta3 = %.3f' %(y3,theta3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "Last let's summarize the error from the various methods.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error (least-squares):\n",
      "   y: 0.170,  theta: 0.232\n",
      "Average absolute error (Bayesian prior = 1):\n",
      "   y: 0.425,  theta: 0.603\n",
      "Average absolute error (Bayesian prior = 2):\n",
      "   y: 0.169,  theta: 0.242\n",
      "Average absolute error (Bayesian prior = 3):\n",
      "   y: 0.131,  theta: 0.195\n"
     ]
    }
   ],
   "source": [
    "x_true   = [y_true, degrees(theta_true)]\n",
    "error_LS = RESULTS_LS - x_true\n",
    "error_B1 = RESULTS_B1 - x_true\n",
    "error_B2 = RESULTS_B2 - x_true\n",
    "error_B3 = RESULTS_B3 - x_true\n",
    "\n",
    "print('Average absolute error (least-squares):')\n",
    "print('   y: %.3f,  theta: %.3f' %tuple( np.abs(error_LS).mean(axis=0) ) )\n",
    "print('Average absolute error (Bayesian prior = 1):')\n",
    "print('   y: %.3f,  theta: %.3f' %tuple( np.abs(error_B1).mean(axis=0) ) )\n",
    "print('Average absolute error (Bayesian prior = 2):')\n",
    "print('   y: %.3f,  theta: %.3f' %tuple( np.abs(error_B2).mean(axis=0) ) )\n",
    "print('Average absolute error (Bayesian prior = 3):')\n",
    "print('   y: %.3f,  theta: %.3f' %tuple( np.abs(error_B3).mean(axis=0) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "These results show that:\n",
    "\n",
    "- Using a vague prior (prior=1) for Baysian IK produces poor results relative to LS-IK results.\n",
    "- Using the LS-IK solution to inform Bayesian priors (prior=2) provides marginal-to-no accuracy improvement. \n",
    "- Using highly informative priors (prior=3) yields highly accurate Bayesian IK results; however, since the true solutions are unknown in practice (if they were, IK wouldn't be necessary), this approach is irrelevant to real (non-simulated) IK problems.\n",
    "\n",
    "In summary, Bayesian IK accuracy is directly related to the information content of the priors. Using an LS-informed prior is not expected to yield accuracy improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
